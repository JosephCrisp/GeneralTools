lines(x=range(values), y=range(values), lty=2, col="black")
for(row in 1:nrow(table)){
lines(x=c(table[row, "Before"], table[row, "Before"]),
y=c(table[row, "Before"], table[row, "After"]),
lty=3, lwd=0.5,
col=ifelse(grepl(pattern="WB", x=table[row, "Isolate"]),
rgb(1,0,0, 0.5), rgb(0,0,1, 0.5)))
}
legend("bottomright", legend=c("Cow", "Badger"),
pch=c(17, 19), cex=1, col=c("blue", "red"),
text.col=c("blue", "red"), bty='n')
after$Species <- "COW"
after$Species[grepl(x=after$Isolate, pattern="WB") == TRUE] <- "BADGER"
after$Species <- as.factor(after$Species)
boxplot(after$Coverage ~ after$Species,
ylim=c(0,1), ylab="Proportion", border=c("red", "blue"),
names=c("Badgers", "Cattle"), outline=FALSE,
las=1, pch=20, main="Isolate Variant Position Coverage")
stripchart(after$Coverage ~ after$Species,
vertical = TRUE, jitter=0.2,
method = "jitter", add = TRUE, pch = 21,
col = c(rgb(1,0,0, 0.5), rgb(0,0,1, 0.5)),
bg=rgb(0.5,0.5,0.5, 0.5))
#  plot(table$Before, table$After, las=1, xlab="Before", ylab="After",
#       main="Difference between before and after rescue",
#       ylim=range(values), xlim=range(values),
#       pch=ifelse(grepl(pattern="WB", x=table$Isolate), 19, 17),
#       col=ifelse(grepl(pattern="WB", x=table$Isolate),
#                  rgb(1,0,0, 0.5), rgb(0,0,1, 0.5)))
#  lines(x=range(values), y=range(values), lty=2, col="black")
#  text(table$Before, table$After,
#       labels=table$Isolate, cex=0.5)
}
lookForDuplicatedWBsToIdentifyResequencedBadgers <- function(coverage){
wbIDs <- list()
# Note the sequence IDs for each WB ID
for(row in 1:nrow(coverage)){
if(grepl(x=coverage[row, "Isolate"], pattern="WB") == TRUE){
id <- strsplit(coverage[row, "Isolate"], split="_")[[1]][1]
if(is.null(wbIDs[[id]]) == TRUE){
wbIDs[[id]] <- c(coverage[row, "Isolate"])
}else{
wbIDs[[id]] <- c(wbIDs[[id]], coverage[row, "Isolate"])
}
}
}
# Examine resequenced WB IDs
resequencedIsolatesToRemove <- c()
for(wbID in names(wbIDs)){
if(length(wbIDs[[wbID]]) > 1){
# Get the coverage of each isolate
coverageValues <- getCoverage(wbIDs[[wbID]], coverage)
# Find the index of the max
maxIndex <- which.max(coverageValues)
# Add the other(s) into array to remove
for(i in 1:length(wbIDs[[wbID]])){
if(i != maxIndex){
resequencedIsolatesToRemove[length(resequencedIsolatesToRemove) + 1] <-
wbIDs[[wbID]][i]
}
}
}
}
return(resequencedIsolatesToRemove)
}
examineCultureIsolatesOfCattleChooseWhenResequenced <- function(cultureIDs, coverage){
resequencedIsolatesToRemove <- c()
for(cultureID in names(cultureIDs)){
if(length(cultureIDs[[cultureID]]) > 1){
# Get the coverage of each isolate
coverageValues <- getCoverage(cultureIDs[[cultureID]], coverage)
# Find the index of the max
maxIndex <- which.max(coverageValues)
# Add the other(s) into array to remove
for(i in 1:length(cultureIDs[[cultureID]])){
if(i != maxIndex){
resequencedIsolatesToRemove[length(resequencedIsolatesToRemove) + 1] <-
cultureIDs[[cultureID]][i]
}
}
}
}
return(resequencedIsolatesToRemove)
}
getCoverage <- function(isolates, coverage){
values <- c()
for(i in 1:length(isolates)){
values[i] <- coverage[which(coverage$Isolate == isolates[i]),
"Coverage"]
}
return(values)
}
createHashtableOfCultureIDs <- function(linkTable){
cultureIDs <- list()
for(row in 1:nrow(linkTable)){
if(is.null(cultureIDs[[linkTable[row, "Strain.ID"]]]) == TRUE){
cultureIDs[[linkTable[row, "Strain.ID"]]] <- c(linkTable[row, "Seq.number"])
}else{
cultureIDs[[linkTable[row, "Strain.ID"]]] <- c(cultureIDs[[linkTable[row, "Strain.ID"]]],
linkTable[row, "Seq.number"])
}
}
return(cultureIDs)
}
addIsolatesToRemove <- function(ids, reason, table){
for(i in 1:length(ids)){
table[1 + nrow(table), ] <- c(ids[i], reason)
}
return(table)
}
parseIsolateColumn <- function(column){
ids <- c()
for(i in 1:length(column)){
parts <- strsplit(column[i], split="_")[[1]]
if(grepl(x=column[i], pattern="WB") == TRUE){
ids[i] <- paste(parts[1], "_", parts[2], sep="")
}else{
ids[i] <- parts[1]
}
}
return(ids)
}
path <- "C:/Users/Joseph Crisp/Desktop/UbuntuSharedFolder/Woodchester_CattleAndBadgers/NewAnalyses_13-07-17/"
###############################################################
# Read in variant position coverage - before and after rescue #
###############################################################
# Read in the genome coverage file
file <- paste(path, "vcfFiles/IsolateVariantPositionCoverage_27-09-2017.txt", sep="")
before <- read.table(file, header=TRUE, sep="\t", stringsAsFactors=FALSE)
file <- paste(path, "vcfFiles/IsolateVariantPositionCoverage_RESCUED_27-09-2017.txt", sep="")
after <- read.table(file, header=TRUE, sep="\t", stringsAsFactors=FALSE)
# Parse the Isolate columns
before$Isolate <- parseIsolateColumn(before$Isolate)
after$Isolate <- parseIsolateColumn(after$Isolate)
#############################
# Plot the isolate coverage #
#############################
# Add species column
after$Species <- "COW"
after$Species[grepl(x=after$Isolate, pattern="WB") == TRUE] <- "BADGER"
# Open a pdf
file <- paste(substr(file, 1, nchar(file) - 4), ".pdf", sep="")
pdf(file)
produceSummaryPlots(before, after)
dev.off()
plotNSamplesPerCounty <- function(countyCoords, countyNames, nSamplesPerCounty){
par(mar=c(0,0,0,0))
plot(x=NULL, y=NULL, yaxt="n", xaxt="n", ylab="", xlab="", bty="n",
xlim=c(countyCoords[["min"]][1], countyCoords[["max"]][1]),
ylim=c(countyCoords[["min"]][2], countyCoords[["max"]][2]))
for(key in names(countyCoords)){
if(key %in% c("min", "max")){
next
}
nSamples <- nSamplesPerCounty[[countyNames[[key]]]]
if(length(nSamples) == 0){
nSamples <- 0
}
polygon(countyCoords[[key]], border=rgb(0,0,0, 1),
col=rgb(0,0,1,nSamples / maxCount),
lwd=2)
if(nSamples != 0){
text(x=mean(countyCoords[[key]][, 1]),
y=mean(countyCoords[[key]][, 2]),
labels=paste(countyNames[[key]], " (", nSamples, ")", sep=""),
cex=0.6, col="red")
}else{
text(x=mean(countyCoords[[key]][, 1]),
y=mean(countyCoords[[key]][, 2]),
labels=countyNames[[key]], cex=0.6, col="gray50")
}
}
}
writePolygonCoordsToFile <- function(countyCoords, countyNames, path){
for(key in names(countyCoords)){
if(key %in% c("min", "max")){
next
}
file <- paste(path, "PolygonCoords_", countyNames[[key]], ".txt", sep="")
table <- countyCoords[[key]]
colnames(table) <- c("X", "Y")
write.table(table, file, sep="\t",
row.names=FALSE, quote=FALSE)
}
}
getValues <- function(list){
values <- c()
for(key in names(list)){
values[length(values) + 1] <- list[[key]]
}
return(values)
}
invertList <- function(list){
output <- list()
for(key in names(list)){
output[[as.character(list[[key]])]] <- key
}
return(output)
}
countNumberSamplesPerCounty <- function(vntrInfo){
counties <- list()
for(row in 1:nrow(vntrInfo)){
county <- strsplit(vntrInfo[row, "Herd Location"], split=" ")[[1]][1]
if(is.null(counties[[county]]) == TRUE){
counties[[county]] <- 1
}else{
counties[[county]] <- counties[[county]] + 1
}
}
return(counties)
}
getPolygonNames <- function(polygonInfo, column){
names <- list()
rowNames <- rownames(polygonInfo)
for(row in 1:nrow(polygonInfo)){
names[[rowNames[row]]] <- as.character(polygonInfo[row, column])
}
return(names)
}
removeSep <- function(string, sep){
parts <- strsplit(x=string, split=sep)[[1]]
return(paste(parts, collapse=""))
}
getPolygonCoords <- function(spatialPolygonsDataFrame){
polygonCoords <- list()
polygonCoords[["min"]] <- c(NA, NA)
polygonCoords[["max"]] <- c(NA, NA)
for(i in 1:length(spatialPolygonsDataFrame@polygons)){
polygonCoords[[spatialPolygonsDataFrame@polygons[[i]]@ID]] <-
spatialPolygonsDataFrame@polygons[[i]]@Polygons[[1]]@coords
rangeX <- range(spatialPolygonsDataFrame@polygons[[i]]@Polygons[[1]]@coords[, 1])
rangeY <- range(spatialPolygonsDataFrame@polygons[[i]]@Polygons[[1]]@coords[, 2])
if(is.na(polygonCoords[["min"]][1]) == TRUE ||
rangeX[1] < polygonCoords[["min"]][1]){
polygonCoords[["min"]][1] <- rangeX[1]
}
if(is.na(polygonCoords[["max"]][1]) == TRUE ||
rangeX[2] > polygonCoords[["max"]][1]){
polygonCoords[["max"]][1] <- rangeX[2]
}
if(is.na(polygonCoords[["min"]][2]) == TRUE ||
rangeY[1] < polygonCoords[["min"]][2]){
polygonCoords[["min"]][2] <- rangeY[1]
}
if(is.na(polygonCoords[["max"]][2]) == TRUE ||
rangeY[2] > polygonCoords[["max"]][2]){
polygonCoords[["max"]][2] <- rangeY[2]
}
}
return(polygonCoords)
}
##################
# Load Libraries #
##################
library(maptools) # Read shape file
library(rgeos) # Polygon centroids
###################################
# Read in ROI counties shape file # https://www.townlands.ie/page/download/
###################################
# Set the path
path <- "C:/Users/Joseph Crisp/Desktop/UbuntuSharedFolder/Paratuberculosis/"
# Read in the shape file
file <- paste(path, "CountyPolygonCoordsROI_CountyBoundaries/counties.shp", sep="")
countyBorders <- readShapePoly(file) # Generates SpatialPolygonsDataFrame
# Get the coordinates of the counties
countyCoords <- getPolygonCoords(countyBorders)
# Get the county names associated with the coords
countyNames <- getPolygonNames(countyBorders@data, "NAME_TAG")
#########################
# Read in MAP VNTR data #
#########################
# Read in the file
file <- paste(path, "Genotyping data.csv", sep="")
vntrInfo <- read.table(file, header=TRUE, sep=",", stringsAsFactors=FALSE,
check.names=FALSE)
# Remove rows with no DNA extraction
vntrInfo <- vntrInfo[vntrInfo[, "Date DNA extraction"] != "", ]
# Count number of samples per county
nSamplesPerCounty <- countNumberSamplesPerCounty(vntrInfo)
maxCount <- max(getValues(nSamplesPerCounty))
#####################
# Plot the counties #
#####################
plotNSamplesPerCounty(countyCoords, countyNames, nSamplesPerCounty)
path <- "C:/Users/Joseph Crisp/Desktop/UbuntuSharedFolder/Paratuberculosis/"
# Read in the shape file
file <- paste(path, "CountyPolygonCoords/CountyPolygonCoordsROI_CountyBoundaries/counties.shp", sep="")
countyBorders <- readShapePoly(file) # Generates SpatialPolygonsDataFrame
# Get the coordinates of the counties
countyCoords <- getPolygonCoords(countyBorders)
# Get the county names associated with the coords
countyNames <- getPolygonNames(countyBorders@data, "NAME_TAG")
path <- "C:/Users/Joseph Crisp/Desktop/UbuntuSharedFolder/Paratuberculosis/"
# Read in the shape file
file <- paste(path, "CountyPolygonCoords/counties.shp", sep="")
countyBorders <- readShapePoly(file) # Generates SpatialPolygonsDataFrame
path <- "C:/Users/Joseph Crisp/Desktop/UbuntuSharedFolder/Paratuberculosis/"
# Read in the shape file
file <- paste(path, "CountyPolygonCoords/ROI_CountyBoundaries/counties.shp", sep="")
countyBorders <- readShapePoly(file) # Generates SpatialPolygonsDataFrame
countyCoords <- getPolygonCoords(countyBorders)
# Get the county names associated with the coords
countyNames <- getPolygonNames(countyBorders@data, "NAME_TAG")
file <- paste(path, "Genotyping data.csv", sep="")
vntrInfo <- read.table(file, header=TRUE, sep=",", stringsAsFactors=FALSE,
check.names=FALSE)
vntrInfo <- vntrInfo[vntrInfo[, "Date DNA extraction"] != "", ]
# Count number of samples per county
nSamplesPerCounty <- countNumberSamplesPerCounty(vntrInfo)
maxCount <- max(getValues(nSamplesPerCounty))
head(countyCoords)
countyNames
plotNSamplesPerCounty(countyCoords, countyNames, nSamplesPerCounty)
printSequence <- function(consensus, fileName){
# Open an output file
connection <- file(fileName, "w")
# Write a sequence header
writeLines(">Consensus", connection)
# Write out the sequence
writeLines(paste(consensus, collapse=""), connection)
# Close the output file
close(connection)
}
getConsensus <- function(sequences){
# Get the sequence names
sequenceNames <- names(sequences)
# Initialise vector to store the consensus
consensus <- c()
# Examine each position in the alignment
for(position in 1:length(sequences[[1]])){
# Initialise a vector to store the alleles at the current position
alleles <- c()
# Examine the current position in each sequence - store each allele
for(i in 1:length(sequenceNames)){
alleles[i] <- sequences[[sequenceNames[i]]][position]
}
# Call the consensus allele
consensus[position] <- "N"
if(length(unique(alleles)) == 1){
consensus[position] <- alleles[1]
# If dash present at current position - return allele present as lower case
}else if(length(unique(alleles)) == 2 && "-" %in% alleles){
consensus[position] <- tolower(alleles[alleles != "-"])
# If N present at current position - return other allele as lower case
}else if(length(unique(alleles)) == 2 && "N" %in% alleles){
consensus[position] <- tolower(alleles[alleles != "N"])
}
}
return(consensus)
}
readFASTA <- function(fastaFile){
# Open the file and store lines
connection <- file(fastaFile, "r")
fileLines <- readLines(connection)
close(connection)
# Initialise a list to store the sequences
sequences <- list()
# Examine each of the file lines one by one
for(i in 1:length(fileLines)){
# Check if line starts with ">"
if(grepl(fileLines[i], pattern="^>") == TRUE){
# Store the previous sequence
if(i != 1){
sequences[[name]] <- strsplit(sequence, split="")[[1]]
}
# Initialise variables to store sequence and id
name <- substr(fileLines[i], 2, stop=nchar(fileLines[i]))
sequence <- ""
}else{
sequence <- paste(sequence, fileLines[i], sep="")
}
}
# Store the last sequence
sequences[[name]] <- strsplit(sequence, split="")[[1]]
return(sequences)
}
library(ape)
# Set the path
path <- "C:/Users/Joseph Crisp/Desktop/"
# Read in the sequences
fastaFile <- paste(path, "test.fasta", sep="")
sequences <- read.FASTA(fastaFile)
# Run the alignment
alignment <- clustal(sequences)
output <- paste(path, "alignment.fasta", sep="")
write.dna(alignment, file = output, format = 'fasta', colsep="")
# Read in the alignment
sequences <- readFASTA(output)
pos <- 1
# Initialise a vector to store the alleles at the current position
alleles <- c()
# Examine the current position in each sequence - store each allele
for(i in 1:length(sequenceNames)){
alleles[i] <- sequences[[sequenceNames[i]]][position]
}
# Get the sequence names
sequenceNames <- names(sequences)
# Initialise a vector to store the alleles at the current position
alleles <- c()
# Examine the current position in each sequence - store each allele
for(i in 1:length(sequenceNames)){
alleles[i] <- sequences[[sequenceNames[i]]][position]
}
position <- 1
# Initialise a vector to store the alleles at the current position
alleles <- c()
# Examine the current position in each sequence - store each allele
for(i in 1:length(sequenceNames)){
alleles[i] <- sequences[[sequenceNames[i]]][position]
}
alleles
# Get the number of each allele present
alleleCounts <- table(alleles)
alleleCounts
which(alleleCounts == max(alleleCounts))
which(alleleCounts == max(alleleCounts))[[1]]
class(which(alleleCounts == max(alleleCounts)))
colnames(alleleCounts)
names(alleleCounts)
alleles <- c("a", "a", "t", "t")
# Get the number of each allele present
alleleCounts <- table(alleles)
# Note the consensus
maxIndex <- which(alleleCounts == max(alleleCounts))
maxIndex
alleleCounts
names(which(alleleCounts == max(alleleCounts)))
alleles <- c(a", "t")
alleles <- c("a", "t")
# Get the number of each allele present
alleleCounts <- table(alleles)
# Note the consensus
maxAllele <- names(which(alleleCounts == max(alleleCounts)))
maxAllele
printSequence <- function(consensus, fileName){
# Open an output file
connection <- file(fileName, "w")
# Write a sequence header
writeLines(">Consensus", connection)
# Write out the sequence
writeLines(paste(consensus, collapse=""), connection)
# Close the output file
close(connection)
}
getConsensus <- function(sequences){
# Get the sequence names
sequenceNames <- names(sequences)
# Initialise vector to store the consensus
consensus <- c()
# Examine each position in the alignment
for(position in 1:length(sequences[[1]])){
# Initialise a vector to store the alleles at the current position
alleles <- c()
# Examine the current position in each sequence - store each allele
for(i in 1:length(sequenceNames)){
alleles[i] <- sequences[[sequenceNames[i]]][position]
}
# Get the number of each allele present
alleleCounts <- table(alleles)
# Note the consensus
maxAllele <- names(which(alleleCounts == max(alleleCounts)))
if(length(maxAllele) == 1){
consensus[position] <- maxAllele
}else{
consensus[position] <- "N"
}
}
return(consensus)
}
readFASTA <- function(fastaFile){
# Open the file and store lines
connection <- file(fastaFile, "r")
fileLines <- readLines(connection)
close(connection)
# Initialise a list to store the sequences
sequences <- list()
# Examine each of the file lines one by one
for(i in 1:length(fileLines)){
# Check if line starts with ">"
if(grepl(fileLines[i], pattern="^>") == TRUE){
# Store the previous sequence
if(i != 1){
sequences[[name]] <- strsplit(sequence, split="")[[1]]
}
# Initialise variables to store sequence and id
name <- substr(fileLines[i], 2, stop=nchar(fileLines[i]))
sequence <- ""
}else{
sequence <- paste(sequence, fileLines[i], sep="")
}
}
# Store the last sequence
sequences[[name]] <- strsplit(sequence, split="")[[1]]
return(sequences)
}
# Run the alignment
alignment <- clustal(sequences)
# Print out the file
output <- paste(path, "alignment.fasta", sep="")
write.dna(alignment, file = output, format = 'fasta', colsep="")
################################
# Build the consensus sequence #
################################
# Read in the alignment
sequences <- readFASTA(output)
# Get the consensus sequence
consensusSequence <- getConsensus(sequences)
# Print out the consensus sequence
outputFile <- paste(path, "Consensus.fasta", sep="")
printSequence(consensusSequence, outputFile)
?write.dna
?read.FASTA
