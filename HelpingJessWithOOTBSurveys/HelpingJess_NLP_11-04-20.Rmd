---
title: "Working on Jessâ€™s transcripts and questionnaires"
author: "Joseph Crispell"
date: "13/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r preparation, include=FALSE}
# Load required libraries
library(textreadr) # Reading text from docx files
library(wordcloud) # Creating word clouds
library(tm) # Used by wordcloud
library(basicPlotteR) # Multiple overlaid histograms
library(topicmodels) # Topic modelling
library(slam) # Need for topic modelling

# Set the path
path <- file.path("~", "Desktop", "HelpingJess")
```

```{r question class and associated functions, echo=FALSE}
# Create a question object to store the data associated with each question
question <- function(name, question, nLinesToSkipAfterQuestion=0, multiLine=FALSE, lineAtFinish=NULL){
              
  # Create the question object and store the input information
  value <- list("name"=name,
                "question"=question,
                "nLinesToSkipAfterQuestion"=nLinesToSkipAfterQuestion,
                "lineAtFinish"=lineAtFinish,
                "answer"=c())

  # Label as question
  attr(value, "class") <- "question"
 
  return(value)
}

# Function to extract answers for questions from file lines
extractAnswers <- function(questions, fileLines){
  
  # Examine each line in the fileLines
  for(lineIndex in seq_along(fileLines)){
    
    # Get the current line
    line <- fileLines[lineIndex]
    
    # Check if current lines matches a question
    questionName <- NA
    for(question in questions){
      
      # Note the question if current line matches question
      if(grepl(line, pattern=question$question)){
        questionName <- question$name
        break
      }
    }
    
    # Skip to next line if current line doesn't match question
    if(is.na(questionName)){
      next
    }
    
    ## Get the answer for the question found
    
    # Note the start line
    startLine <- lineIndex + 1 + questions[[questionName]]$nLinesToSkipAfterQuestion
      
    # Identify the last line of the answer
    lastLine <- lineIndex + which(grepl(fileLines[(lineIndex+1):length(fileLines)], 
                                        pattern=questions[[questionName]]$lineAtFinish,
                                        fixed=TRUE)) - 1
      
    # Store the answer
    questions[[questionName]]$answer <- fileLines[startLine:lastLine]
    
    # Jump to line after current question
    lineIndex <- lastLine + 1
  }
  
  return(questions)
}
```

```{r define question structure, echo=FALSE}
# Note each question of interest
questions <- list(
  "hours"=question(name="hours", 
                   question="Roughly how many hours do you spend at AMS in an average week?",
                   lineAtFinish="Please indicate your normal times at Abbeymount studios"),
  "location"=question(name="location",
                      question="How does the location of Abbeymount Studios impact on your work?",
                      lineAtFinish="Do you have any connections to the local community and/or to local businesses?"),
  "connections"=question(name="connections",
                         question="Do you have any connections to the local community and/or to local businesses?",
                         lineAtFinish="Partnership/collaboration"),
  "issues"=question(name="issues",
                    question="What are the issues and challenges you face with your work?",
                    lineAtFinish="What have your responses and adaptations to these challenges been?"),
  "responses"=question(name="responses",
                       question="What have your responses and adaptations to these challenges been?",
                       lineAtFinish="Future Development"),
  "future"=question(name="future",
                    question="How would you like to see Out of the Blue Abbeymount Studios develop in the future?",
                    lineAtFinish="Would you be interested in retail space at AMS?"),
  "retail"=question(name="retail",
                    question="Would you be interested in retail space at AMS?",
                    lineAtFinish="Would you be interested in exhibition space at AMS?"),
  "exhibition"=question(name="exhibition",
                        question="Would you be interested in exhibition space at AMS?",
                        lineAtFinish="What have been your successes and highlights whilst working at Abbeymount?"),
  "value"=question(name="value",
                   question="To help us convey the value and impact of Abbeymount Studios, if you have time we would really appreciate it if you are able to to tell us highlights and successes at AMS",
                   lineAtFinish="We would like to interview a representative cross section of Abbeymount residents across the widely varying practices.")
)
```

```{r general functions, echo=FALSE}

comparisonWordCloud <- function(sentences, sentimentLexicon){
  
  # Categorise and score the sentiment for all words from all answers to the exhibition question
  words <- getSentiment(sentences, sentimentLexicon)

  # Create a table for a comparison word cloud
  tableForComparison <- data.frame("Negative"=ifelse(words$sentiment == "negative", words$count, 0),
                                   "Positive"=ifelse(words$sentiment == "positive", words$count, 0))
  rownames(tableForComparison) <- words$word
  tableForComparison <- tableForComparison[is.na(tableForComparison$Negative) == FALSE, ]
  
  # Create a comparison word cloud
  comparison.cloud(as.matrix(tableForComparison))
}

getSentiment <- function(sentences, sentimentLexicon){

    # Extract all words from input sentences
  words <- extractWordsFromSentence(sentences, wordSizeThreshold=1)
  
  # Count how many times each word appears
  wordCounts <- table(words)
  words <- data.frame("word"=names(wordCounts), "count"=as.numeric(wordCounts))
  
  # Assign a sentiment (postive or negative) to each word
  words$sentiment <- as.factor(sentimentLexicon[words$word, "sentiment"])
  words$score <- sentimentLexicon[words$word, "value"]

  return(words)
}

calculateSentimentScore <- function(sentences, sentimentLexicon){

  # Initialise a vector to store the sentiment score for each sentence
  sentimentScores <- c()
  
  # Examine each of sentences
  for(index in seq_along(sentences)){
    
    # Get the words from the current sentence
    words <- extractWordsFromSentence(sentences[index], wordSizeThreshold=1)
    
    # Calculate the sum of the sentiment score for all the words
    sentimentScores[index] <- sum(sentimentLexicon[words, "value"], na.rm=TRUE)
  }
  
  return(sentimentScores)
}

extractWordsFromSentence <- function(sentences, wordSizeThreshold){
  
  # Convert to lower case
  sentences <- tolower(sentences)
  
  # Replace new line characters with spaces
  sentences <- gsub(pattern="\n", replacement=" ", sentences)
  
  # Extract the words from the sentence
  arrayOfWords <- unlist(strsplit(sentences, " "))

  # Remove non-character characters from words
  arrayOfWords <- gsub(pattern="[[:punct:]]", replacement="", x=arrayOfWords)

  # Ignore words with fewer than X characters
  arrayOfWords <- arrayOfWords[nchar(arrayOfWords) >= wordSizeThreshold]
  
  return(arrayOfWords)
}

readQuestionnairesAndExtractAnswers <- function(folder, questions){
  
  # Find all files in folder
  files <- list.files(folder)
  
  # Get the word documents
  files <- files[grepl(pattern=".docx$", files)]
  
  # Ignore temporary files
  files <- files[grepl(pattern="^~", files) == FALSE]
  
  # Initialise a list to store the answers from each file
  answers <- questions
  answers$files <- files
  
  # Extract answers from each survey response
  for(fileIndex in seq_along(files)){
    
    # Read in the lines from a questionnaire
    questionnaireFile <- file.path(folder, files[fileIndex])
    questionnaire <- read_docx(questionnaireFile)

    # Extract the answers to the questions of interest
    questionnairesAnswers <- extractAnswers(questions, questionnaire)
    
    # Store the answers from the current file
    for(question in names(questions)){
      
      # Combine a multi-line answer into single string
      answer <- ifelse(is.null(questionnairesAnswers[[question]]$answer), 
                       " ", 
                       paste(questionnairesAnswers[[question]]$answer, collapse="\n"))
      
      # Check if haven't already added answers for current question
      if(is.null(answers[[question]]$answer)){
        answers[[question]]$answer <- rep("", length(files))
      }
      
      # Store the current answer to the current question from the current file
      answers[[question]]$answer[fileIndex] <- answer
    }
  }
  
  return(answers)
}
```

```{r read in questionnaires, echo=FALSE}
# Note the questionnaires folder
# Google drive folder: 
folder <- file.path(path, "Questionnaires")

# Read in the questionnaires and extract answers
answers <- readQuestionnairesAndExtractAnswers(folder, questions)
```

```{r value of Abbeymount Studios, echo=FALSE}
# Extract the words from the answers about the value of OOTB
words <- extractWordsFromSentence(answers$value$answer, wordSizeThreshold=4)

# Create a word cloud
wordcloud(words, max.words=500, min.freq=2, scale=c(2.5,0.75), colors=brewer.pal(8, "Dark2"))
```

```{r load AFINN lexicon, echo=FALSE}

# Read in the latest table for github
url <- "https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt"
sentimentLexicon <- read.table(url, header=FALSE, stringsAsFactors=FALSE, sep="\t", quote="")

# Add column and row names
colnames(sentimentLexicon) <- c("word", "value")
rownames(sentimentLexicon) <- sentimentLexicon$word

# Add binary sentiment column
sentimentLexicon$sentiment <- NA
sentimentLexicon$sentiment[sentimentLexicon$value > 0] <- "positive"
sentimentLexicon$sentiment[sentimentLexicon$value < 0] <- "negative"
sentimentLexicon$sentiment <- as.factor(sentimentLexicon$sentiment)
```

```{r sentiment analyses, echo=FALSE}
# Calculate the sentiment score for each answer to the exhibition space question
sentimentScoresExhibition <- calculateSentimentScore(answers$exhibition$answer, sentimentLexicon)

# Calculate the sentiment score for each answer to the retail space question
sentimentScoresRetail <- calculateSentimentScore(answers$retail$answer, sentimentLexicon)

# Plot the sentiment scores
scores <- list(sentimentScoresExhibition, sentimentScoresRetail)
plotMultipleHistograms(scores, nBins=10, xlim=c(-6, 6), 
                       colours=c(rgb(1,0,0, 0.5), rgb(0,0,1, 0.5)),
                       las=1, xlab="Sentiment score",
                       main="Comparing sentiment of responses for retail\n or exhibition space at AMS")
legend("topleft", 
       legend=c(paste0("Retail space (average=", median(sentimentScoresRetail), ")"),
                paste0("Exhibition space (average=", median(sentimentScoresExhibition), ")")),
       text.col=c("blue", "red"), bty="n")

# Create a comparison cloud using the words from the answers
comparisonWordCloud(answers$exhibition$answer, sentimentLexicon)

# Create a comparison cloud using the words from the answers
comparisonWordCloud(answers$retail$answer, sentimentLexicon)
```

```{r topic modelling, echo=FALSE}

# Build a Latent Dirichlet allocation topic model
ldaModel <- LDA(termFreq(answers$value$answer), k=2)

slotNames(ldaModel)
slot(ldaModel, "alpha")
slot(ldaModel, "loglikelihood")
table(as.matrix(slot(ldaModel, "wordassignments"))) 

# Extract the topic matrix
topicMatrix <- tidy(ldaModel, matrix="beta")


```

```{r generating n-grames, echo=FALSE}

```


